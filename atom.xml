<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>sulami's blog</title>
    <link href="https://blog.sulami.xyz/atom.xml" rel="self" />
    <link href="https://blog.sulami.xyz" />
    <id>https://blog.sulami.xyz/atom.xml</id>
    <author>
        <name>Robin Schroer</name>
        <email>blog@peerwire.org</email>
    </author>
    <updated>2021-05-06T00:00:00Z</updated>
    <entry>
    <title>Onboarding Across Timezones</title>
    <link href="https://blog.sulami.xyz/posts/onboarding-across-timezones/" />
    <id>https://blog.sulami.xyz/posts/onboarding-across-timezones/</id>
    <published>2021-05-06T00:00:00Z</published>
    <updated>2021-05-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Even in a fully distributed organisation, teams are often clustered in timezones to facilitate collaboration. Another model is the deliberate spreading of teams to enable <em><a href="https://en.wikipedia.org/wiki/Follow-the-sun">Follow the Sun</a></em> workflows, which can also improve pager rotations.</p>
<p>In a distributed team, onboarding new team members can be especially difficult. New onboardees lack the context and institutional knowledge required to work effectively in an independent manner. Here are some strategies which I have found ease this process, specifically in the field of software engineering.</p>
<h2 id="onboarding-buddies">Onboarding Buddies</h2>
<p>It is standard practice to designate an onboarding buddy for a new onboardee, a person in the same team who can take care of them during the first few weeks, answer questions, and pair a lot. It is not always possible to have this onboarding buddy in the same timezone, and in these cases it is a good idea to have an existing employee in the same timezone as a designated point of contact, regardless of team affiliations.</p>
<h2 id="making-the-most-of-the-overlap">Making the Most of the Overlap</h2>
<p>When onboarding someone in different timezones, everything has to be planned around the overlap in office hours. Start by clearly identifying this overlap. It is most important for the onboardee to be able to onboard and work effectively outside of this overlap.</p>
<p>To optimise the value gotten out of the overlap, it is best spent with synchronous conversations. Other tasks such as code reviews or planning work should be moved outside the overlap unless there is value in doing it synchronously, for example by sharing additional context.</p>
<p>If the overlap is very small, short screencasts can be medium-cost, high-bandwidth method to transfer context asynchronously. These can be recorded and watched outside the overlap, but convey much of the same context as pairing could, albeit without the ability to actually interject questions.</p>
<h2 id="enabling-self-directed-learning">Enabling Self-Directed Learning</h2>
<p>To enable the onboardee to learn on the job, there needs to be a pool of work items for them to pick up tasks from. These items need be narrowly scoped and include much more detail and context than they ordinarily would, to reduce the risk of the onboardee getting stuck.</p>
<p>Tasks should be sized “just right,” that is not larger than two or three days, but also not shorter than a single day. Very small tasks encourage the onboardee to start working on several tasks in parallel while waiting for code reviews, increasing cognitive load.</p>
<p>A library of self-directed training material is a must-have for every organisation, distributed or not, but can also serve as a fallback if the onboardee gets stuck on their current task and has to wait for the next overlap.</p>
<h2 id="just-because-you-can-does-not-mean-you-should">Just Because You Can Does Not Mean You Should</h2>
<p>While asking colleagues for ideas &amp; feedback, a sentiment I have heard more than once was “if you can, avoid scattering your team.” Just because you can spread a team literally around the globe does not mean you should do this. At my current day job we have employees around the globe, but generally try to keep teams within two timezone regions, for example within the North American east coast and Europe, or the west coast and APAC, so that we have an overlap of at least two to three hours.</p>]]></summary>
</entry>
<entry>
    <title>The Shape of Tests</title>
    <link href="https://blog.sulami.xyz/posts/test-shapes/" />
    <id>https://blog.sulami.xyz/posts/test-shapes/</id>
    <published>2021-03-03T00:00:00Z</published>
    <updated>2021-03-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Many tests for an operation iterate over a mapping of different inputs to expected outcomes. By looking at the tests for a single operation as the same test with different inputs and output expectations, we can start to question how we should model those tests.</p>
<h2 id="tests-as-matrices">Tests as Matrices</h2>
<p>By simply enumerating every possible combination of input values, we can construct a matrix with as many dimensions as inputs. We can then define the expected result for each set of inputs, and write a generalised test function:</p>
<blockquote>
<p>∀ input∈{(a, b, …, n) | a∈A, b∈B, …, n∈N} f(input)</p>
</blockquote>
<p>The number of possible test cases is thus:</p>
<blockquote>
<p>|<em>i</em><em>n</em><em>p</em><em>u</em><em>t</em><em>s</em>| = |A × B × ⋯ × N|</p>
</blockquote>
<p>As soon as our operation accepts an input that has more than a few possible values, that is any kind of number, string, or complex data structure, enumerating every possible input combination becomes impractical. Instead we can resort to groupings of values via properties.</p>
<p>This is a test matrix for division which uses properties instead of values, with the rows being dividends, and the columns divisors:</p>
<table>
<thead>
<tr class="header">
<th>÷</th>
<th>Positive</th>
<th>Zero</th>
<th>Negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Positive</td>
<td>Positive</td>
<td>undefined</td>
<td>Negative</td>
</tr>
<tr class="even">
<td>Zero</td>
<td>Zero</td>
<td>undefined</td>
<td>Zero</td>
</tr>
<tr class="odd">
<td>Negative</td>
<td>Negative</td>
<td>undefined</td>
<td>Positive</td>
</tr>
</tbody>
</table>
<p>Matrices like this are necessarily exhaustive, and force us to think about the result for every possible combination of the input values we have included.</p>
<p>This is an implementation of the same property matrix in Clojure:</p>
<div class="sourceCode"><pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">ns</span> division-matrix-test
  (<span class="at">:require</span> [clojure.<span class="kw">test</span> <span class="at">:refer</span> [<span class="bu">deftest</span><span class="fu"> is </span><span class="kw">testing</span>]]
            [clojure.spec.alpha <span class="at">:as</span> s]
            [clojure.spec.gen.alpha <span class="at">:as</span> gen]))

(<span class="bu">defn</span><span class="fu"> safe-divide</span>
  <span class="st">&quot;2-arity `/`, but returns `nil` on division by zero.&quot;</span>
  [dividend divisor]
  (<span class="kw">try</span>
    (<span class="kw">/</span> dividend divisor)
    (<span class="kw">catch</span> ArithmeticException _
      nil)))

(<span class="bu">defmacro</span><span class="fu"> test-matrix</span>
  <span class="st">&quot;Generates tests for a two-dimensional test matrix.&quot;</span>
  [test-fn matrix]
  (<span class="kw">let</span> [columns (<span class="kw">rest</span> (<span class="kw">first</span> matrix))
        rows (<span class="kw">map</span> <span class="kw">first</span> (<span class="kw">rest</span> matrix))
        combinations (<span class="kw">for</span> [[row idy] (<span class="kw">map</span> #(<span class="kw">vector</span> %<span class="dv">1</span> %<span class="dv">2</span>) rows (<span class="kw">range</span>))
                           [col idx] (<span class="kw">map</span> #(<span class="kw">vector</span> %<span class="dv">1</span> %<span class="dv">2</span>) columns (<span class="kw">range</span>))]
                       [row col (<span class="kw">-&gt;</span> matrix
                                    (<span class="kw">nth</span> (<span class="kw">inc</span> idy))
                                    (<span class="kw">nth</span> (<span class="kw">inc</span> idx)))])]
    `(<span class="kw">doseq</span> [combination# [<span class="at">~@combinations]]</span>
       (<span class="kw">apply</span> <span class="at">~test-fn</span> combination#))))

(<span class="bu">deftest</span><span class="fu"> safe-division-test</span>
  (<span class="kw">let</span> [gen-input
        (<span class="kw">fn</span> [kind]
          (<span class="kw">case</span> kind
            <span class="at">:pos</span> (gen/generate (s/gen pos-int?))
            <span class="at">:neg</span> (gen/generate (s/gen neg-int?))
            <span class="at">:zero</span> <span class="dv">0</span>))]

    (test-matrix

     (<span class="kw">fn</span> [x y result-pred]
       (<span class="kw">let</span> [dividend (gen-input x)
             divisor (gen-input y)]
         (<span class="kw">is</span> (result-pred (safe-divide dividend divisor))
             (<span class="kw">format</span> <span class="st">&quot;Failed with: %s / %s&quot;</span> dividend divisor))))

     [[nil    <span class="at">:pos</span>   <span class="at">:zero</span>  <span class="at">:neg</span> ]
      [<span class="at">:pos</span>   <span class="kw">pos?</span>   <span class="kw">nil?</span>   <span class="kw">neg?</span> ]
      [<span class="at">:zero</span>  <span class="kw">zero?</span>  <span class="kw">nil?</span>   <span class="kw">zero?</span>]
      [<span class="at">:neg</span>   <span class="kw">neg?</span>   <span class="kw">nil?</span>   <span class="kw">pos?</span> ]])))</code></pre></div>
<p>In this case we are testing a safe variant of the division function <code>/</code>, which returns <code>nil</code> if the divisor is zero. This simplifies the testing process, because we do not have to include any exception catching logic in our test function, or invent a notation to mean <em>this set of inputs should result in a thrown exception</em>.</p>
<p>It is worth noting that such a direct interpretation of a matrix is only possible in a language as malleable as Clojure. In other languages, we might have to resort to enumerating a set of <code>(dividend
divisor result)</code> tuples, losing the guarantee of covering all possible combinations.</p>
<p>But even in Clojure, more than two dimensions in this matrix will quickly become unwieldy and hard to follow, and a tuple-based approach would scale better to larger numbers of input parameters.</p>
<h2 id="tests-as-trees">Tests as Trees</h2>
<p>Another way we could structure our tests is as a tree. A tree does not have to be exhaustive the same way a matrix has to be. We can omit certain combinations of inputs by pruning their branches. In this way we are implying that if a single input has a given value, it defines the result regardless of the other inputs’ values.</p>
<p>In the division example all branches with a divisor of zero could be collapsed into a single case, as the dividend does not matter in this case. This only works if the first level of branching describes the divisor, and the dividends are on the second level.</p>
<div class="sourceCode"><pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">ns</span> division-tree-test
  (<span class="at">:require</span> [clojure.<span class="kw">test</span> <span class="at">:refer</span> [<span class="kw">are</span> <span class="bu">deftest</span><span class="fu"> testing</span>]]
            [clojure.spec.alpha <span class="at">:as</span> s]
            [clojure.spec.gen.alpha <span class="at">:as</span> gen]))

(<span class="bu">deftest</span><span class="fu"> safe-division-test</span>

  (<span class="kw">testing</span> <span class="st">&quot;with a positive divisor&quot;</span>
    (<span class="kw">let</span> [divisor (gen/generate (s/gen pos-int?))]

      (<span class="kw">testing</span> <span class="st">&quot;and a positive dividend&quot;</span>
        (<span class="kw">let</span> [dividend (gen/generate (s/gen pos-int?))]
          (<span class="kw">is</span> (<span class="kw">pos?</span> (safe-divide dividend divisor)))))

      (<span class="kw">testing</span> <span class="st">&quot;and a zero dividend&quot;</span>
        (<span class="kw">let</span> [dividend <span class="dv">0</span>]
          (<span class="kw">is</span> (<span class="kw">zero?</span> (safe-divide dividend divisor)))))

      (<span class="kw">testing</span> <span class="st">&quot;and a negative dividend&quot;</span>
        (<span class="kw">let</span> [dividend (gen/generate (s/gen neg-int?))]
          (<span class="kw">is</span> (<span class="kw">neg?</span> (safe-divide dividend divisor)))))))

  (<span class="kw">testing</span> <span class="st">&quot;with a divisor of zero&quot;</span>
    (<span class="kw">let</span> [dividend (gen/generate (s/gen int?))]
      (<span class="kw">is</span> (<span class="kw">nil?</span> (safe-divide dividend <span class="dv">0</span>)))))

  (<span class="kw">testing</span> <span class="st">&quot;with a negative divisor&quot;</span>
    (<span class="kw">let</span> [divisor (gen/generate (s/gen neg-int?))]

      (<span class="kw">testing</span> <span class="st">&quot;and a positive dividend&quot;</span>
        (<span class="kw">let</span> [dividend (gen/generate (s/gen pos-int?))]
          (<span class="kw">is</span> (<span class="kw">neg?</span> (safe-divide dividend divisor)))))

      (<span class="kw">testing</span> <span class="st">&quot;and a zero dividend&quot;</span>
        (<span class="kw">let</span> [dividend <span class="dv">0</span>]
          (<span class="kw">is</span> (<span class="kw">zero?</span> (safe-divide dividend divisor)))))

      (<span class="kw">testing</span> <span class="st">&quot;and a negative dividend&quot;</span>
        (<span class="kw">let</span> [dividend (gen/generate (s/gen neg-int?))]
          (<span class="kw">is</span> (<span class="kw">pos?</span> (safe-divide dividend divisor))))))))</code></pre></div>
<p>This might look more verbose, but in exchange we get a unique label for every tree branch, which can improve readability. The nesting also naturally lends itself to lexical scoping, so we only have the values in scope which apply on a given branch.</p>
<p>A key advantage of the tree structure is flexibility. If one of the branches requires special code, we can confine it to that branch, avoiding complicating the remaining branches more than necessary.</p>
<p>Trees also scale better with larger numbers of inputs or options for inputs. A tree might grow overly wide or deep, but we can split it if that becomes a problem.</p>
<p>There is a downside to omitting branches though. If we change our <code>safe-divide</code> function to return different results depending on the dividend when the divisor is zero, our tests might still pass, depending on the specific inputs used, but we will lose test coverage for certain code paths. We have chosen to not test certain input combinations, and we need to be aware of this omission when we are changing the code under test.</p>
<h2 id="tests-as-definitions">Tests as Definitions</h2>
<p>Considering the formula describing the generalised test function above, we could also consider translating this directly into code. This can work, but only if we can test results without re-implementing large parts of the code under test, otherwise we are overly coupling the tests to the code. In the division case, we can decide the sign of the result based on the signs of the inputs.</p>
<div class="sourceCode"><pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="kw">ns</span> division-spec-test
  (<span class="at">:require</span> [clojure.<span class="kw">test</span> <span class="at">:refer</span> [<span class="bu">deftest</span><span class="fu"> is</span>]]
            [clojure.spec.alpha <span class="at">:as</span> s]
            [clojure.spec.<span class="kw">test</span>.alpha <span class="at">:as</span> stest]))

(<span class="bu">defn-</span><span class="fu"> check-safe-divide-result </span>[{{<span class="at">:keys</span> [dividend divisor]} <span class="at">:args</span>
                                  ret <span class="at">:ret</span>}]
  (<span class="kw">cond</span>
    (<span class="kw">zero?</span> divisor) (<span class="kw">nil?</span> ret)

    (<span class="kw">zero?</span> dividend) (<span class="kw">zero?</span> ret)

    (<span class="kw">or</span> (<span class="kw">and</span> (<span class="kw">pos?</span> dividend) (<span class="kw">pos?</span> divisor))
        (<span class="kw">and</span> (<span class="kw">neg?</span> dividend) (<span class="kw">neg?</span> divisor)))
    (<span class="kw">pos?</span> ret)

    <span class="at">:else</span> (<span class="kw">neg?</span> ret)))

(s/fdef safe-divide
  <span class="at">:args</span> (s/cat <span class="at">:dividend</span> <span class="kw">number?</span>
               <span class="at">:divisor</span> <span class="kw">number?</span>)
  <span class="at">:ret</span> (s/nilable <span class="kw">number?</span>)
  <span class="at">:fn</span> check-safe-divide-result)

(<span class="bu">deftest</span><span class="fu"> safe-divide-spec-test</span>
  (<span class="kw">let</span> [check-result (stest/check `safe-divide)]
    (<span class="kw">is</span> (<span class="kw">not</span> check-result)
        (<span class="kw">format</span> <span class="st">&quot;Failed with: %s&quot;</span>
                (<span class="kw">-&gt;</span> check-result
                    <span class="kw">first</span>
                    stest/abbrev-result
                    <span class="at">:failure</span>
                    <span class="at">::stest</span>/val)))))</code></pre></div>
<p>This solution is specific to Clojure, though many other languages have property based testing tools that work similarly.</p>
<p>By adding a spec to our function, we can run a large number of different inputs against our function, and assert a property about the result based on the inputs. It will even shrink the inputs to find the simplest set of inputs to trigger a spec failure.</p>
<p>This means we do not have a programmer writing a matrix or a tree by hand anymore, which has some advantages. The main one being that a programmer might not consider all possible inputs.</p>
<div class="sourceCode"><pre class="sourceCode clojure"><code class="sourceCode clojure">Fail in safe-divide-spec-test
Failed with: {<span class="at">:args</span> {<span class="at">:dividend</span> ##NaN, <span class="at">:divisor</span> <span class="dv">0</span>}, <span class="at">:ret</span> ##NaN}

Fail in safe-divide-spec-test
Failed with: {<span class="at">:args</span> {<span class="at">:dividend</span> <span class="dv">1</span>, <span class="at">:divisor</span> ##Inf}, <span class="at">:ret</span> <span class="fl">0.0</span>}

Fail in safe-divide-spec-test
Failed with: {<span class="at">:args</span> {<span class="at">:dividend</span> <span class="fl">6.812735744013041E-108</span>, <span class="at">:divisor</span> <span class="fl">2.7578261315509936E216</span>}, <span class="at">:ret</span> <span class="fl">0.0</span>}</code></pre></div>
<h2 id="conclusion">Conclusion</h2>
<p>The optimal shape of a test depends mainly on the structure of the inputs to the operation we are testing, as well as its nature.</p>
<p>For pure functions which we expect to use widely and change rarely, property-based testing can be desirable to avoid unintended consequences. There is also a certain speed requirement for test shrinking to work effectively.</p>
<p>Operations with a small number of possible inputs can also be tested via test matrices, which have fewer limitations, but do not guarantee correctness, as only the programmer can assert the completeness of the matrix. They are easy to extend with additional values for parameters, but harder to extend with additional values. Their declarative nature can be useful for documentation purposes.</p>
<p>At the other end of the spectrum, tree-shaped tests are the most flexible, and scale best for larger operations with many inputs. If different branches require fundamentally different setup, test trees can isolate that complexity to where it is required. They also require the most care to keep tidy, and have a tendency to sprawl if unsupervised.</p>]]></summary>
</entry>
<entry>
    <title>Traps to Avoid When Reviewing Code Changes</title>
    <link href="https://blog.sulami.xyz/posts/code-reviews/" />
    <id>https://blog.sulami.xyz/posts/code-reviews/</id>
    <published>2021-01-18T00:00:00Z</published>
    <updated>2021-01-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Reviewing code changes is an underappreciated art. It is part of most software engineers’ daily routine, but as an industry we do little towards developing it as a skill, even though it contributes directly to the quality of the software we produce.</p>
<h2 id="the-lgtm-trap">The LGTM Trap</h2>
<p>Characterised by the eponymous review comment, this trap can have different root causes, all of them resulting in the rubber-stamping of a bad pull request.</p>
<p>First of all, conducting proper code reviews is difficult and mentally exhausting. It is important to take breaks when conducting long or consecutive reviews. Resist the temptation to just “get something in” because it has been open for a while, or because someone else is blocked. Avoid including anything that you already know will need fixing up later, this road leads to <a href="https://blog.codinghorror.com/the-broken-window-theory/">broken windows</a>. This also means your comment-change-test cycle should be as fast as possible to encourage fixing even the smallest issues before merging.</p>
<p>If a critical issue makes it past your review, you should investigate how it got missed, and what you can do to catch similar issues in the future. This way you can build your own checklist to use during reviews. You can also ask someone you trust to conduct a supplementary review, or even try pairing with them on some reviews.</p>
<h2 id="the-human-linter-trap">The Human Linter Trap</h2>
<p>Engineering time is expensive, and the focus required for good reviews is hard to maintain, so minimising the time required for a review is key. This is why we should avoid automatable tasks in code reviews. Prime examples include linting and enforcing a style guide. A <a href="https://www.git-scm.com/docs/githooks">pre-commit hook</a> or a CI job can do either of these much more efficiently than a human reviewer ever could.</p>
<p>Beyond the efficiency gains, this also avoids the clutter resulting from many small comments pointing out typos, bad indentation, or non-idiomatic code, and lets both the reviewer and the author focus on more important issues.</p>
<h2 id="the-implementation-trap">The Implementation Trap</h2>
<p>Tests can not only be useful to ensure correctness of our code, they can also help us during review. Tests exercise the interface of our code, ideally without giving us too much of an idea of the implementation. As a general rule, you want to be reviewing changes from the outside in.</p>
<p>This forces you to actually understand the test code, assert that the checks performed actually match the contract you expect the code to abide by, and catch potential holes in test coverage. It also allows you to judge the interface provided, as awkward tests often hint at sub-optimally factored code.</p>
<h2 id="the-iceberg-trap">The Iceberg Trap</h2>
<p><sup>7</sup>⁄<sub>8</sub> ths of an iceberg are famously below the water line and functionally invisible. Similarly some of the most important parts to pay attention to during reviews are not visible in the diff. This can range from introducing some avoidable duplication because the author was not aware of existing code with the same functionality, all the way to production outages because a remote piece of code made an assumption that does not hold anymore.</p>
<p>It can be helpful to checkout the change locally and look at it in the context of the entire code base instead of in isolation. Asking others familiar with the code base or related ones to have a cursory look can also uncover a wide range of problems quickly.</p>
<h2 id="the-rube-goldberg-trap">The Rube Goldberg Trap</h2>
<p>Just because you can, it does not mean you should. And sometimes there is a better solution than the one proposed.</p>
<p>To review a change, it is important to agree on the problem to solve. Ask the author to supply a problem statement if it is not presented in the change. Only once you understand the problem statement you can evaluate the quality of a solution. The solution could be over- or under-engineered, implemented in the wrong place, or you could even disagree with the problem statement altogether.</p>]]></summary>
</entry>
<entry>
    <title>Lightning Introduction to Nix for Developers</title>
    <link href="https://blog.sulami.xyz/posts/nix-for-developers/" />
    <id>https://blog.sulami.xyz/posts/nix-for-developers/</id>
    <published>2020-11-27T00:00:00Z</published>
    <updated>2020-11-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h2 id="motivation">Motivation</h2>
<p>This is the part where <a href="https://www.youtube.com/watch?v=Unzc731iCUY">Patrick Winston says</a> I need to promise something you will gain by reading this. My promise is the following: This is a very quick introduction to setup Nix on a macOS (or Linux) machine, which will get you reproducible, sandboxed environments to do your development in, as well as atomic rollbacks.</p>
<p>There are many introductions to Nix, but this one aims for speed. I will be skipping over a lot of the fundamentals and only tell you what you absolutely necessary to get up and running. Nix is a large and complex system, but you can get some returns on your time investment within 15 minutes, and decide on delving in deeper later.</p>
<p>This guide is aimed at macOS, but most of it can be applied to Linux as well.</p>
<h2 id="homebrew">Homebrew?</h2>
<p>The de-facto standard package manager for macOS is <a href="https://brew.sh/">Homebrew</a>. While it is a passable solution for installing Mac apps, it has a few shortcomings, some of which can be especially problematic for developers.</p>
<dl>
<dt>Sandboxing</dt>
<dd>Homebrew packages are for the most part installed into <code>/usr/local/bin</code>, which means they are always available to everyone. This can lead to conflicts which can require manual modification of <code>$PATH</code> to resolve. Especially programming languages tend to hit this, as some software only runs on specific versions of their language.
</dd>
<dt>Freezing</dt>
<dd>Even though Homebrew is based on git, is does not support explicitly installing specific versions of a package, or pinning the version in a lockfile. This by extension also means that whenever you install a homebrew on a new system, you will not be able to reproduce the exact versions installed on a known good system.
</dd>
<dt>Patching</dt>
<dd>If you want to modify a package, you have to do it manually after installing, and potentially after every update. Homebrew can apply patches during the build process (via <code>brew
 edit</code>), but again, there is no declarative way of doing so.
</dd>
</dl>
<p>Both the freezing problem and the patching problem can be circumvented by maintaining <a href="https://docs.brew.sh/Taps">your own tap</a>, but this comes with a significant maintenance burden, and I would not recommend it.</p>
<p>I would like to note that you likely cannot replace Homebrew entirely by Nix, as a lot of macOS-exclusive apps are not packaged in Nixpkgs. You could probably package them yourself if you really wanted to, but this has the same problems as maintaining your own Homebrew tap.</p>
<h2 id="installing-nix">Installing Nix</h2>
<p>Before we can use it, of course we have to install Nix. I am using macOS, so I will also install nix-darwin. If you are using Linux, you can install <a href="https://github.com/nix-community/home-manager">home-manager</a> instead for a declarative system setup.</p>
<h3 id="nix">Nix</h3>
<p>To get started, first we install the Nix package manager and language itself.</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">curl</span> -L https://nixos.org/nix/install <span class="op">&gt;</span> /tmp/install-nix.sh
<span class="fu">less</span> /tmp/install-nix.sh  # inspect the script
<span class="fu">sh</span> /tmp/install-nix.sh --darwin-use-unencrypted-nix-store-volume</code></pre></div>
<p>The extra argument is specific to newer Macs with a T2 chip. Refer to <a href="https://nixos.org/manual/nix/stable/#ch-installing-binary">the manual</a> for more details.</p>
<h3 id="nix-darwin">Nix-Darwin</h3>
<p>Next we install <a href="https://github.com/LnL7/nix-darwin">nix-darwin</a>, which is essentially a framework written in the Nix language. It establishes a declarative configuration for the whole system, which packages are installed, all the way to <a href="https://macos-defaults.com/">defaults</a>. One of my personal selling points is management of <a href="https://developer.apple.com/library/archive/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/CreatingLaunchdJobs.html">Launch Agents</a> in Nix, which is much nicer to manage than writing XML and working with <code>launchctl</code>.</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">nix-build</span> https://github.com/LnL7/nix-darwin/archive/master.tar.gz -A installer
<span class="ex">./result/bin/darwin-installer</span></code></pre></div>
<p>The installer will prompt us with a few questions along the way, which do not seem to be well documented. Generally we want to respond with <code>y</code> throughout (the first one is optional).</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">Would</span> you like edit the default configuration.nix before starting? [y/n]
<span class="ex">Would</span> you like to manage <span class="op">&lt;</span>darwin<span class="op">&gt;</span> with nix-channel? [y/n]
<span class="ex">Would</span> you like to load darwin configuration in /etc/bashrc? [y/n]
<span class="ex">Would</span> you like to create /run? [y/n]</code></pre></div>
<p>At this point we are all set. We might need to start up a new shell to load the newly installed commands. If everything worked, we should now have <code>darwin-rebuild</code> in our <code>$PATH</code>.</p>
<h2 id="declaring-the-system">Declaring the System</h2>
<p>The first use case we will be looking at is using Nix to setup our system as a whole.</p>
<h3 id="how-nix-works">How Nix Works</h3>
<p>I will interrupt here for a brief (and simplified) explanation of how Nix works in the first place. Essentially Nix works by building and installing software according to a set of recipes (Nix expressions) in what is called the Nix store, which is just a directory at <code>/nix</code>. To actually make the software available, it creates symbolic links to into the store in a profile, which is just another directory. This profile can then be added to <code>$PATH</code>, so that we can just use the software installed. The beauty of the symbolic links is that we can create many profiles which link to different sets and/or different versions of software in the store.</p>
<p>This also allows us to version profiles, and switch atomically between them, because every time we run <code>darwin-rebuild switch</code>, a new profile is created and activated. Should anything break, we can just switch back to the old profile. In practice this means running <code>darwin-rebuild --rollback</code>. We can also switch to a specific version, using <code>--list-generations</code> and <code>--switch-generation</code> if we want to rollback more than one change.</p>
<h3 id="installing-a-package">Installing a Package</h3>
<p>Before we can install a package, we need to find it first. Finding a package is as simple as running</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">nix</span> search some-package</code></pre></div>
<p>Let us modify <code>$HOME/.nixpkgs/darwin-configuration.nix</code> now. If we open that file, we should find a section similar to this:</p>
<div class="sourceCode"><pre class="sourceCode nix"><code class="sourceCode bash"><span class="ex">environment.systemPackages</span> =
  [ <span class="ex">pkgs.vim</span>
  ];</code></pre></div>
<p>This is where nix-darwin declares the packages installed on the system. Go ahead and add a package to that list. Nix does not use commas to separate list items, just whitespace. The canonical package to add is <code>pkgs.ripgrep</code>, but any will do. Rebuild the system:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">darwin-rebuild</span> switch</code></pre></div>
<p>We should now have <code>rg</code> in our <code>$PATH</code>, without having to open a new shell, as <code>$PATH</code> did not actually change. The <a href="https://daiderd.com/nix-darwin/manual/index.html">nix-darwin manual</a> has a big list of configuration options that might also be interesting, but are not required now.</p>
<h3 id="fetching-updates">Fetching Updates</h3>
<p>As mentioned above, anything we build and install is controlled by our local Nix expressions in the Nix store. These are just build recipes in the Nix language, similar to Makefiles. The expressions usually pin a specific version of the software they build, and they themselves are also versioned. This means to update our packages, we need to update the expressions, which we do like so:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">nix-channel</span> --update</code></pre></div>
<p>This fetches the latest versions of all channels we follow and updates our local Nix expressions accordingly. If a software definition got updated upstream, we can now rebuild it to get the updated version. Because channels are also versioned, we can even rollback channel updates if an upstream update broke for us.</p>
<p>To actually rebuild the packages according to the new definitions, we have to build a new version of our profile:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">darwin-rebuild</span> switch</code></pre></div>
<h2 id="using-nix-shell">Using nix-shell</h2>
<p>There is another way of using Nix than installing all packages system-wide. If we just want to try out a package without having to rebuild our system (and reverting afterwards), we can simply run</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">nix-shell</span> -p some-package</code></pre></div>
<p>Nix will build the package in the Nix store and drop us into a shell that has access to the package. Add <code>--pure</code>, and we get a completely clean environment except for anything that we explicitly add to the shell. This can be useful if the mere existence of a system-wide piece of software is problematic.</p>
<p>If we use this method to setup a Nix environment for a specific project, we can use a <code>shell.nix</code> file to declaratively express the environment like so:</p>
<div class="sourceCode"><pre class="sourceCode nix"><code class="sourceCode bash"><span class="kw">{</span> <span class="ex">pkgs</span> ? import <span class="op">&lt;</span>nixpkgs<span class="op">&gt;</span> <span class="dt">{}</span> <span class="kw">}</span>:
<span class="ex">pkgs.mkShell</span> {
  <span class="ex">buildInputs</span> = with pkgs<span class="kw">;</span><span class="bu"> [</span>
    python-2.6
  ];
  PYTHONDONTWRITEBYTECODE <span class="ot">=</span> <span class="st">&quot;1&quot;</span>;
}</code></pre></div>
<p>This looks complicated, but this does two simple things: anything in <code>buildInputs</code> is made available to the shell, and anything else is injected as an environment variable. Just calling <code>nix-shell</code> in the same directory will automatically pick up this file and execute it.</p>
<p>In this case we are simply getting an older version of Python, and also setting a related environment variable. Anyone using this configuration will have the same environment, which mirrors some of the benefits of Docker, but without the overhead of running containers.</p>
<h2 id="how-to-debug-problems">How to Debug Problems</h2>
<p>This is the hard bit about Nix, the documentation is almost infamously sparse, and common recommendations are to either find an existing solution for your problem, or to read the Nix code involved. Because Nix includes a whole programming language, it allows users to build their own abstractions, which means that many packages have their own way of doing things.</p>
<p>I wish I could provide a sure way to solving all your Nix-related problems, but a lot of it comes back to pasting error messages into search engines and asking people online. Nix is not without its rough edges, and sooner or later you will run into one of them. I consider them learning opportunities, but they can be very frustrating.</p>
<h2 id="where-to-go-from-here">Where to Go From Here</h2>
<p>This is just the beginning, there are many more parts of Nix to discover. It is probably advisable to read through the <a href="https://nixos.org/guides/nix-pills/index.html">Nix Pills</a> to get a better understanding of the language and system.</p>
<p><a href="https://github.com/nix-community/home-manager">home-manager</a> is a project which manages a per-user Nix environment in a declarative way. If you are using nix-darwin it is somewhat optional, but can still be useful to build a more portable configuration. It can be installed as a nix-darwin plugin as well.</p>
<p>If you are looking for better project environment management with Nix, there are a few very useful tools. <a href="https://github.com/nmattia/niv">Niv</a> allows you to declare and pin dependencies for a project. <a href="https://github.com/target/lorri">Lorri</a> is a daemon that automates a lot of the <code>nix-shell</code> setup we have been doing by hand above, such as automatically loading and reloading an environment when you enter a project directory. <a href="https://direnv.net/">Direnv</a> and <a href="https://github.com/Shopify/shadowenv">shadowenv</a> are alternatives to lorri.</p>
<p>You might also want to try packaging some of your own software in Nix, or software that is not in <a href="https://github.com/NixOS/nixpkgs">Nixpkgs</a> (yet). It is good exercise to gain a deeper understanding of the system, and as a bonus you get a more reproducible setup. Nix is a great fit to distribute internal developer tooling as well. I might write something on how to do this in the future.</p>
<p>Last but not least, if you really enjoy using Nix, you might want to try running NixOS, a whole Linux distribution which is configured using Nix.</p>]]></summary>
</entry>
<entry>
    <title>Writing for Reasons</title>
    <link href="https://blog.sulami.xyz/posts/writing-for-reasons/" />
    <id>https://blog.sulami.xyz/posts/writing-for-reasons/</id>
    <published>2020-11-08T00:00:00Z</published>
    <updated>2020-11-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>This year, I have been writing more than even before over. In this article, I would like to discuss some of the reasons for writing and provide some thoughts on each.</p>
<h2 id="writing-to-remember">Writing to Remember</h2>
<p>This is probably the most obvious reason to write for a lot of people. Having written down a piece of information, you can come back later and recall it. Historical context can be invaluable for decision making, and often covers information that is not readily available anymore.</p>
<p>The key here is being able to find notes later on. Paper-based ones can be sorted by topic or chronologically, digital ones can be searched for. Formats can be useful here too, for example by supporting embedded code blocks or graphics.</p>
<h2 id="writing-to-solve-problems">Writing to Solve Problems</h2>
<p>Early this year, before the pandemic hit Europe, I saw Paulus Esterhazy’s talk <em><a href="https://www.youtube.com/watch?v=T7-2DW-KDV4&amp;t=1429s">Angels Singing: Writing for Programmers</a></em> at <a href="https://clojured.de/">clojureD</a>. It contained this great quote of Milton Friedman:</p>
<blockquote>
<p>If you cannot state a proposition clearly and unambiguously, you do not understand it.</p>
</blockquote>
<p>In <a href="https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/HammockDrivenDev.md">another talk</a>, Rich Hickey explained his notion of using notes as an extension of his working memory:</p>
<blockquote>
<p>So we have a problem, in general, because we’re just being asked to write software that’s more and more complex as time goes by. And we know there’s a 7 +/- 2 sort of working memory limit and as smart as any of us are, we all suffer from the same limit but the problems that we are called upon to solve are much bigger than that. So what do we do if we can’t think the whole thing in our head at the same time? How can we work on a problem with more than nine components. What I’m going to recommend is that you write all the bits down.</p>
<p>[…]</p>
<p>But if we look at the 7 +/- 2 thing, we could say we can juggle seven to nine balls but if you can imagine having an assistant who every now and then can take one of those out and put a different color in and you can juggle balls of 20 different colors at the same time as long as there are only nine in the air at any one point in time. And that’s what you’re doing, you’re going to sort of look around at all these pieces and shift arbitrary shapes of seven into your head at different points in time.</p>
</blockquote>
<p>Writing everything down allows digging deep into details and going off on tangents, and then returning to other aspects. As an added bonus, these notes can be useful in the future as well, if archived properly. I found <a href="https://orgmode.org/features.html">org-mode</a> outlines incredibly powerful for this purpose, with their foldable, tree-like structure that allows nesting sub-problems.</p>
<h2 id="writing-to-make-decisions">Writing to Make Decisions</h2>
<p>Writing is invaluable for decision making. Not only does it aid the decision process (see above), it also allows returning to a decision later and reviewing it.</p>
<p><a href="https://github.com/joelparkerhenderson/architecture_decision_record">Architecture decision records (ADRs)</a> are a tool established just for this purpose. The exact formats vary, and the details do not matter too much, but here are a few key points I consider essential:</p>
<ul>
<li>The motivation for the decision</li>
<li>The constraints involved</li>
<li>The alternatives to consider and their respective tradeoffs</li>
</ul>
<p>All of these are useful in several ways: they force you to acknowledge the components of the decision, make it simple to get an opinion on the matter from someone else, and also allow you to review the (potentially bad) decision later on.</p>
<p>There is one more point: the conclusion. This is easy to forget, because once a conclusion is reached, no one wants to spend time writing it down. But if you do not write it down, the document does not tell the whole story if reviewed in the future.</p>
<h2 id="writing-to-develop-ideas">Writing to Develop Ideas</h2>
<p>This year I have seen a lot of people writing about Sönke Ahrens’ <a href="https://takesmartnotes.com/"><em>How to Take Smart Notes</em></a>, which is about taking notes as a means to develop long form writing. It popularised the idea of the <em>Zettelkasten</em>, a physical or virtual box of notes which reference each other to build an information network.</p>
<p>While I found the book quite interesting, I would not recommend it to everyone due to the significant organisation overhead involved.</p>
<p>That being said, I believe that if you have a digital system which can provide automatic back-links to avoid the exponentially growing amount of manual maintenance required, there is little harm in linking notes. At the very least it will make it easier to find a note, and maybe it can aid the thinking process by exposing previously unseen connections between concepts.</p>
<h2 id="writing-to-communicate">Writing to Communicate</h2>
<p>This very article was written expressively to communicate information, and as such required some extra work for it to be effective.</p>
<p>The most important factor when writing for communication is the target audience. It dictates the format to use, and which prior knowledge can be assumed. Maximising information density by being as concise as possible is important to avoid wasting the reader’s time.</p>
<p>As an added difficulty, when writing something to be published you need to get it right the first time, there is no channel for discussing follow-up questions. The old adage in writing is “writing is rewriting”, and I very much believe that to be true in this case. Write an outline, then a first draft, then keep reading and revising it until it is just right. Maybe show it to someone you trust for feedback.</p>
<p>I personally also like to leave a draft and come back a few weeks later. This way I always have a few drafts for new articles ready for revision, until I feel that one is ready for publishing.</p>]]></summary>
</entry>
<entry>
    <title>Testing Hexagonal Architecture</title>
    <link href="https://blog.sulami.xyz/posts/testing-hexagonal-architecture/" />
    <id>https://blog.sulami.xyz/posts/testing-hexagonal-architecture/</id>
    <published>2020-10-11T00:00:00Z</published>
    <updated>2020-10-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><a href="https://web.archive.org/web/20180822100852/http://alistair.cockburn.us/Hexagonal+architecture"><em>Hexagonal Architecture</em></a>, also known as <em>Ports and Adapters</em>, was first conceived by Cockburn in 2005, and popularised by Freeman &amp; Pryce’s <a href="http://www.growing-object-oriented-software.com/"><em>Growing Object-Oriented Software, Guided by Tests</em></a> in 2009. For those unfamiliar, it describes an application architecture entirely comprised of ports, which are interfaces, and adaptors, which are implementations for those interfaces. The adaptors can depend on other ports, but not on other adaptors. A system is then constructed by selecting a full set of adaptors, depending on the requirements, and composing them using <a href="https://en.wikipedia.org/wiki/Dependency_injection">dependency injection</a>.</p>
<p>A port can represent an external resource or service, but also a logical component of the system, like an HTTP server or a queue handler.</p>
<h2 id="an-example-port-adaptor">An Example Port &amp; Adaptor</h2>
<p>A simple example for a port could be blob storage. I will be using Clojure in this post, but no prior knowledge is required for understanding. A port in this case is a protocol, which we implement like so:</p>
<div class="sourceCode"><pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="bu">defprotocol</span><span class="fu"> BlobStoragePort</span>
  (store-object [this loc obj]
    <span class="st">&quot;Store `obj` at  `loc`.&quot;</span>)
  (retrieve-object [this loc]
    <span class="st">&quot;Retrieve the object at `loc`.</span>
<span class="st">    Returns `nil` if not found.&quot;</span>))</code></pre></div>
<p>Now that we have a port with an interface in the form of abstract method declarations, we can implement an adaptor, for example using S3:</p>
<div class="sourceCode"><pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="bu">defrecord</span><span class="fu"> S3StorageAdaptor </span>[bucket-loc]
  BlobStoragePort
  (store-object [this loc obj]
    (s3/put-object <span class="at">:bucket-loc</span> bucket-loc
                   <span class="at">:key</span> loc
                   <span class="at">:file</span> obj))
  (retrieve-object [this loc]
    (s3/get-object <span class="at">:bucket-loc</span> bucket-loc
                   <span class="at">:key</span> loc)))

(<span class="bu">defn</span><span class="fu"> new-s3-storage-adaptor </span>[bucket-loc]
  (s3/create-bucket bucket-loc)
  (-&gt;S3StorageAdaptor bucket-loc))</code></pre></div>
<p>During tests, we would like to use a blob storage that is much faster and not dependent on external state, so we can use a simple map in an atom:</p>
<div class="sourceCode"><pre class="sourceCode clojure"><code class="sourceCode clojure">(<span class="bu">defrecord</span><span class="fu"> MemoryBlobStorageAdaptor </span>[storage-map]
  BlobStoragePort
  (store-object [this loc obj]
    (<span class="kw">swap!</span> storage-map <span class="kw">assoc</span> loc obj))
  (retrieve-object [this loc]
    (<span class="at">:loc</span> <span class="at">@storage-map)))</span>

(<span class="bu">defn</span><span class="fu"> new-memory-blob-storage-adaptor </span>[]
  (-&gt;MemoryBlobStorageAdaptor (<span class="kw">atom</span> {})))</code></pre></div>
<h2 id="testing-the-port">Testing the Port</h2>
<p>It has been long known that a direct mapping of tests to internal methods is an anti-pattern to be avoided. As such we will prefer testing on a port-level over testing on an adaptor-level. In practice that means we assert a certain set of behaviours about every adaptor for a given port by using only the public port methods in our tests, and using the same tests for all adaptors.</p>
<div class="sourceCode"><pre class="sourceCode clojure"><code class="sourceCode clojure"><span class="co">;; Abstract port test suite</span>

(<span class="bu">defn-</span><span class="fu"> store-and-retrieve-test </span>[adaptor]
  (<span class="kw">testing</span> <span class="st">&quot;store and retrieve returns the object&quot;</span>
    (<span class="kw">let</span> [loc <span class="st">&quot;store-and-retrieve&quot;</span>
          obj <span class="st">&quot;test-object&quot;</span>]
      (store-object adaptor loc obj)
      (<span class="kw">is</span> (<span class="kw">=</span> obj
             (retrieve-object adaptor loc))))))

(<span class="bu">defn-</span><span class="fu"> not-found-test </span>[adaptor]
  (<span class="kw">testing</span> <span class="st">&quot;returns nil for nonexistent objects&quot;</span>
    (<span class="kw">is</span> (<span class="kw">nil?</span> (retrieve-object adaptor <span class="st">&quot;not-found&quot;</span>)))))

<span class="co">;; Specific adaptor tests</span>

(<span class="bu">deftest</span><span class="fu"> blob-storage-adaptor-test</span>
  (<span class="kw">let</span> [adaptors [(new-memory-blob-storage-adaptor)
                  (new-s3-blob-storage-adaptor <span class="st">&quot;test&quot;</span>)]]
    (<span class="kw">for</span> [adaptor adaptors]
      (store-and-retrieve-test adaptor)
      (not-found-test adaptor))))</code></pre></div>
<p>This has the advantage of establishing a consistent set of behaviours across all adaptors and keeping them in sync. One might wonder about intended behavioural differences between adaptors for the same port, but I would argue that from the outside, all adaptors for a given port should exhibit the same behaviour. Because we are only using the public interface for testing, any internal differences are conveniently hidden from us.</p>
<h2 id="the-rest-of-the-system">The Rest of the System</h2>
<p>Now that we have established a port, as well as some adaptors, we can build on top of them. Blob storage is a lower level ports in our system, and we are going to add a higher level port that implements some kind of business logic which requires blob storage.</p>
<div class="sourceCode"><pre class="sourceCode clojure"><code class="sourceCode clojure"><span class="co">;; Port definition omitted for brevity.</span>

(<span class="bu">defrecord</span><span class="fu"> BusinessLogicAdaptor </span>[blob-storage-adaptor]
  BusinessLogicPort
  (retrieve-double [this loc]
    (<span class="kw">*</span> <span class="dv">2</span> (retrieve-object blob-storage-adaptor loc))))</code></pre></div>
<p>We are free to use different blob storage adaptors for different systems, for example production, staging, CI, or local development. The business logic adaptor is oblivious to the actual blob storage implementation injected.</p>
<h2 id="on-mocks-stubs">On Mocks &amp; Stubs</h2>
<p>The careful reader might have noticed that the dependency injection of different adaptors looks a lot like mocking, and this is very much true. While mocking has been considered more and more problematic in recent years, the fact that we assert the same set of behaviours for our mocks as we assert for the “real components” leads us to much more fully featured and realistic mocks, compared to the ones which are written for specific tests and then rarely touched after.</p>
<p>If the difference in behaviour between different adaptors leads to problems which are not caught by the test suite, the problems is not mocking, but an incomplete behaviour specification for the adaptor in question.</p>]]></summary>
</entry>
<entry>
    <title>Keyboardio Atreus Review</title>
    <link href="https://blog.sulami.xyz/posts/atreus/" />
    <id>https://blog.sulami.xyz/posts/atreus/</id>
    <published>2020-07-10T00:00:00Z</published>
    <updated>2020-07-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I recently received my early bird <a href="https://shop.keyboard.io/products/keyboardio-atreus">Keybardio Atreus</a> from <a href="https://www.kickstarter.com/projects/keyboardio/atreus/description">the Kickstarter</a> and have now been using it for about three weeks, so I am writing a review for folks considering buying one after release.</p>
<h2 id="a-bit-of-history">A Bit of History</h2>
<p>Most of this is also outlined on the Atreus website, but here is the short version: my colleague Phil Hagelberg designed <a href="http://atreus.technomancy.us/">the original Atreus</a> keyboard in 2014, and has been selling kits for self-assembly ever since.</p>
<p>In 2019 Keyboardio, the company which created the <a href="https://www.kickstarter.com/projects/keyboardio/the-model-01-an-heirloom-grade-keyboard-for-seriou">Model 01</a>, got together with Phil to build a pre-assembled commercial version of the Atreus. Their Kickstarter ran earlier in 2020 and collected almost $400k.</p>
<p>Phil’s original 42-key version can be built with either a PCB or <a href="https://www.youtube.com/watch?v=zY2k75eWrLQ">completely hand-wired</a>, and uses a wooden, acrylic, or completely custom (e.g. 3D-printed) case.</p>
<p>Keyboardio split the two larger thumb keys into two regular size keys, bringing the total up to 44, and uses a PCB and Cherry MX-style switches mounted on an Aluminium plate inside a black ABS case.</p>
<h2 id="hardware">Hardware</h2>
<p>At a first impression, it is incredibly small, noticeably smaller still than the small <a href="https://www.apple.com/shop/product/MLA22LL/A/magic-keyboard-us-english">Apple Magic Keyboard</a>. At the same time, it uses a regular key spacing, so once your hands are in place it does not feel cramped at all. On the contrary, every time I use a different keyboard now, I feel that half the keys are too far away to reach comfortably. It is also flat enough that I can use it without a wrist rest.</p>
<p>Mine has <a href="https://cdn.shopify.com/s/files/1/3099/8088/files/CPG151101D213_Copper.pdf?11838687770053773851">Kailh Speed Copper</a> switches, which require 40g of force to actuate, with very early actuation points. They are somewhat comparable to <a href="https://www.cherrymx.de/_Resources/Persistent/13618248706cd28e75ab9bdf9e55e9f8794611c1/EN_CHERRY_MX_BROWN.pdf">Cherry MX Browns</a> without the dead travel before the tactile bump. As mentioned above, the switches are mounted on an aluminium plate, and can be swapped without disassembly.</p>
<p>The early actuation point of the switches does require some getting used to, I keep experiencing some key chatter, especially on my weaker fingers, though Jesse from Keyboardio is working hard on alleviating that.</p>
<p>When it comes to noise, you can hear that it is a mechanical keyboard. Even with relatively quiet switches, the open construction means that the sound of the keys getting released is audible in most environments. I would hesitate to bring it to a public space, like a café or a co-working space. Open-office depends on the general noise level, and how tolerant your coworkers are, I have not had anyone complain about the sound level in video conferences.</p>
<p>The keycaps used are XDA-profile laser-engraved PBT of medium thickness. Apparently there have been a lot of issues with the durability of the labels, so the specifics of that might change. I personally have had a single key start to fade a bit over 3 weeks of use, but I do not actually care.</p>
<p>The keyboard is powered by the <a href="https://www.microchip.com/wwwproducts/en/ATmega32U4">ATmega32U4</a>, which is a pretty standard controller for a keyboard, it is also used in the <a href="https://www.pjrc.com/teensy/">Teensy 2.0</a> for example.</p>
<p>I would judge the overall build quality as good. While it does not feel like an ultra-premium product, there is nothing specific I can actually complain about, no rough edges or manufacturing artefacts.</p>
<h2 id="software">Software</h2>
<p>Out of the box, the keyboard uses the open-source <a href="https://github.com/keyboardio/Kaleidoscope">Kaleidoscope</a> firmware, which can be configured with the (also open-source) <a href="https://github.com/keyboardio/Chrysalis">Chrysalis</a> graphical configurator. Supposedly it is also possible to use <a href="https://qmk.fm/">QMK</a>, and Phil has recently written <a href="https://git.sr.ht/~technomancy/menelaus">Menelaus</a>, a firmware in <a href="https://ryansuchocki.github.io/microscheme/">Microscheme</a>.</p>
<p>I have stuck with (pre-release versions of) Kaleidoscope so far, which has worked out fairly well. Chrysalis is an Electron app, and doing sweeping changes in it can be a bit cumbersome compared to using text-based, declarative configuration, but it does the job. Flashing a new version onto the keyboard only takes a few seconds. I also have to mention the <a href="https://kaleidoscope.readthedocs.io/en/latest/">extensive documentation</a> available. Kaleidoscope has a rich plugin infrastructure, very little of which I actually use, but it does seem to rival QMK in flexibility.</p>
<p>I am using the Atreus with <a href="https://colemak.com/">Colemak</a>, the same layout I have been using for almost a decade now, and compared to trying the <a href="https://ergodox-ez.com/">Ergodox</a>, the switching was much smoother. I am mostly back to my regular typing speed of 80-90 WPM after three weeks, and I can still use a regular staggered layout keyboard without trouble.</p>
<p>The modifier keys at the bottom are unusual, but work for me. I use the three innermost keys with my thumbs, and the bottom edges by just pushing down with my palm. It does require some careful arrangement to avoid often having to press two modifiers on the same time at once.</p>
<p>With only 44 physical keys, the keyboard makes heavy use of layers, which can be temporarily shifted to when holding a key, or switched to permanently. By default the first extra layer has common special characters on the left half, and a numpad on the right, which works better than a regular keyboard for me.</p>
<p>The only problem I sometimes have is the lack of a status indicator. This means I have to keep track of the keyboard state in my head when switching layers. Not a big problem though.</p>
<h2 id="conclusion">Conclusion</h2>
<p>My conclusion is quite simple: if you are in the market for a keyboard like this, this might be the keyboard for you. It does what it does well, and is much cheaper than anything comparable that does not require manual assembly. I personally enjoy the small form factor, the flexible (set of) firmware, and the RSI-friendly layout.</p>
<p>I also want to highlight the truly amazing effort Keyboardio puts into supporting their customers. You can browse the Kickstarter or their GitHub projects to see how much effort they put into this, and I have been in contact with Jesse myself while trying to debug a debouncing issue in the firmware. I am very happy to support them with my wallet.</p>]]></summary>
</entry>
<entry>
    <title>LISP&lt;sub&gt;1&lt;/sub&gt; Has Won</title>
    <link href="https://blog.sulami.xyz/posts/lisp-1/" />
    <id>https://blog.sulami.xyz/posts/lisp-1/</id>
    <published>2020-06-10T00:00:00Z</published>
    <updated>2020-06-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I am currently working on a compiler for a new programming language which has been in the making for a few months at this point. There is nothing public to show yet, everything is very early stage, and there are plenty of decisions to make and work to be done before I will publish anything.</p>
<p>That being said, I will write about both the progress as well as different topics I come across, so <a href="/atom.xml">stay tuned</a> if you are interested in that.</p>
<p>The language I am writing currently has a Lisp-like syntax, because that is easy to parse an work with, which is why I am sharing some thoughts on one of the big bike sheds in software history.</p>
<h2 id="lispwhat">LISP<sub>what?</sub></h2>
<p>LISP<sub>1</sub> and LISP<sub>2</sub> are terms to describe the way symbol namespaces work in different LISP-like programming languages. The explanation is actually very simple, LISP<sub>1</sub> has a single shared namespace for functions and variables. This means a symbol can refer to either a function or a variable, but not both. Consider the following Racket code:</p>
<div class="sourceCode"><pre class="sourceCode scheme"><code class="sourceCode scheme">(<span class="kw">define</span><span class="fu"> </span>(double x)
  (* <span class="dv">2</span> x))

(<span class="kw">define</span><span class="fu"> triple </span>(* <span class="dv">3</span> <span class="dv">4</span>))

double
<span class="co">;; =&gt; #&lt;procedure:double&gt;</span>

triple
<span class="co">;; =&gt; 12</span>

(double triple)
<span class="co">;; =&gt; 24</span></code></pre></div>
<p>When you resolve a symbol to a variable, you cannot know if it will resolve to a function or not.</p>
<p>LISP<sub>2</sub> on the other hand has a separate namespace for functions. This has the advantage that every name can be used twice, once for a function, and once for a variable. The tradeoff is that the user has to specify in which namespace they want to resolve a symbol. Consider the following Emacs Lisp code:</p>
<div class="sourceCode"><pre class="sourceCode commonlisp"><code class="sourceCode commonlisp">(<span class="kw">defun</span><span class="fu"> double </span>(x)
  (<span class="op">*</span> <span class="dv">2</span> x))

(<span class="kw">defvar</span><span class="fu"> double </span>(<span class="op">*</span> <span class="dv">2</span> <span class="dv">4</span>))

(<span class="kw">funcall</span> #&#39;double double)
<span class="co">;; =&gt; (funcall &lt;function double&gt; &lt;variable double&gt;)</span>
<span class="co">;; =&gt; (double 6)</span>
<span class="co">;; =&gt; 12</span></code></pre></div>
<p>Note the added punctuation to denote the first <code>double</code> as a symbol resolving to a function.</p>
<h2 id="lispwhy">LISP<sub>why?</sub></h2>
<p>LISP is one of the oldest programming languages that is still used commercially today in some form, if you accept Common Lisp in its lineage. It appears that the namespace separation in the original LISP 1.5 was mostly incidental, and <a href="http://www.nhplace.com/kent/Papers/Technical-Issues.html">has been regretted since</a>.</p>
<p>The set of LISP<sub>2</sub> languages is quite small these days. Besides Common Lisp and Emacs Lisp, both of which are over three decades old at this point, there are also Ruby and Perl.</p>
<p>The other ancient LISP-like language, Scheme, is a LISP<sub>1</sub>, and so is its popular modern dialect Racket (as demonstrated above). Almost every other somewhat popular language chooses to share a single namespace between functions and variables. Examples include Clojure, Janet, Python, Java, JavaScript, and even Rust.</p>
<p>Clearly the benefits of less syntactic clutter and cognitive overhead have won in the popular arena, to the point that the established de facto standard itself becomes a good reason to stick with a single unified namespace. Of course improvement, by its very definition, always requires change, but language designers need to be acutely aware of the cost incurred by diverging from the established norm.</p>]]></summary>
</entry>
<entry>
    <title>Literate Calculations in Emacs</title>
    <link href="https://blog.sulami.xyz/posts/literate-calc-mode/" />
    <id>https://blog.sulami.xyz/posts/literate-calc-mode/</id>
    <published>2020-05-21T00:00:00Z</published>
    <updated>2020-05-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>It is no secret that I am a big fan of <a href="https://en.wikipedia.org/wiki/Literate_programming">literate programming</a> for many use cases. I think it is a great match for investigative or exploratory notes, research, and <a href="https://github.com/sulami/dotfiles/blob/master/emacs/.emacs/README.org">configuration</a>.</p>
<p>On a Friday evening about two weeks ago, my flatmate came up with an idea for doing calculations in a literate way. Of course, if you really wanted to, you could use a <a href="https://jupyter.org/try">Jupyter Notebook</a>, but we were looking for something more lightweight, and ideally integrated into Emacs.</p>
<p>A quick search came up empty, so on Saturday morning I got started writing what came to be <a href="https://github.com/sulami/literate-calc-mode.el">Literate Calc Mode</a>. The features I wanted included named references to earlier results, spreadsheet-like recalculations on every change, and the ability to save my calculations to a file. And then of course the ability to interlace calculations with explanatory text.</p>
<p>It was in part inspired by the iOS app <a href="http://tydligapp.com/">Tydlig</a>, which also provides calculations with automatically updating references to earlier results, but does not allow saving the workspaces as files, which I find very limiting.</p>
<p>But enough talk, this is what the result looks like in action:</p>
<div class="figure">
<video controls proload="none" alt="Demo Video">
<source src="/raw/literate-calc-demo.webm" type="video/webm">
</video>
</div>
<p>This is <code>literate-calc-minor-mode</code>, running alongside <code>org-mode</code>. As you can see, it automatically picks up calculations and inserts the results as overlays at the end of the line. It allows the user to bind results to variables, which can even have names with spaces. Any change causes all values to be recalculated, similar to a spreadsheet.</p>
<p>Because it uses Emacs’ built-in <code>calc-eval</code> behind the scenes, it supports almost everything <code>M-x calc</code> does, including formulas, complex units, and unresolved mathematical variables.</p>
<p>Of course there are also other convenience functions, such as evaluating just a single line, or inserting the results into the file for sharing. I do have some more plans for the future, which are outlined in the <a href="https://github.com/sulami/literate-calc-mode.el#roadmap">documentation</a>.</p>
<p>In addition to hopefully providing some value to other Emacs users, this was also a great learning experience. I have learned a lot about overlays in Emacs, and I published my first package on <a href="https://melpa.org/">MELPA</a>, which was a thoroughly pleasant experience.</p>]]></summary>
</entry>
<entry>
    <title>The Grumpy Developer&#39;s Guide to Meetings</title>
    <link href="https://blog.sulami.xyz/posts/engineers-meeting-guide/" />
    <id>https://blog.sulami.xyz/posts/engineers-meeting-guide/</id>
    <published>2020-04-21T00:00:00Z</published>
    <updated>2020-04-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>While everyone is writing about remote meetings these days, I do not believe that successful remote meetings are actually meaningfully different from successful in-person meetings.</p>
<p><a href="http://twitchard.github.io/posts/2020-03-28-against-process.html">Like many other developers</a>, I loathe meetings, especially when they seem like a waste of valuable time. My philosophy is “if you have to set up a meeting, at least do it right”. Consequently, here are some rules for successful meetings, both remote and in-person:</p>
<h2 id="the-best-meeting-is-no-meeting">The Best Meeting Is No Meeting</h2>
<p>It is important to understand the tradeoffs involved when deciding to schedule a meeting. Meetings are synchronous discussions, which makes them useful for knowledge exchanges and decision making processes, but they are also time-intensive and disruptive for everyone involved.</p>
<p>The alternative to organising a meeting is an asynchronous conversation, for example on a shared document. This can be faster than a meeting overall, as it allows participants to participate in their own time, instead of trying to find a slot that fits everyone. In addition to that, it is much less disruptive compared scheduling a meeting, especially for <a href="https://www.oreilly.com/library/view/changing-software-development/9780470515044/9780470515044_software_developers_are_knowledge_worker.html">knowledge workers</a>.</p>
<p>The only good reason for a synchronous meeting is making decisions requiring knowledge exchange and/or consensus. A good example would be planning a complicated feature, or cleaning up a backlog.</p>
<h2 id="the-most-important-work-happens-before-the-meeting">The Most Important Work Happens Before the Meeting</h2>
<p>Every meeting needs an organiser. The organiser needs to define the expected outcome of the meeting, as well as gather all required information, and share all of these in the meeting agenda. The outcome should be tangible, like a decision or a set of tasks to be done.</p>
<p>The agenda is crucial, because the participants will likely have to do some preparation in advance to the meeting. If any specific work needs to be done before the meeting, like research or data retrieval, make sure to clearly assign tasks.</p>
<p>I recommend writing out the full agenda before scheduling the meeting, and then sharing the agenda with the calendar invitation. This allows everyone to schedule their preparation in their own time.</p>
<h2 id="the-actual-meeting">The Actual Meeting</h2>
<p>During the meeting it is important to keep notes, usually in a shared document. You can designate a scribe, or just agree to all contribute some bullet points whenever possible. You want to write down any conclusions reached, important points made, and further questions and future work to be done. Do not care too much about form, the meeting organiser should rewrite the notes after the meeting anyway.</p>
<p>The meeting is prime talking time, and you should treat it as such. Do not spend time watching someone carry out a task. If you find during the meeting that you require more information, write this down as a future task instead of derailing the meeting.</p>
<p>The organiser should always work towards the defined outcome, but it often happens that some other discussion emerges as a precondition to reaching the outcome. There is a balance to be struck between discussion necessary to get to a meaningful outcome and veering too far off-topic.</p>
<p>Keep in mind that the time slot scheduled is more a rough guideline than a rule. If you can finish early, do so. If you need more time, and everyone involved has more time, take it. Do not try to force an outcome if you did not get there in time. If you find at the start of the meeting that some necessary precondition has not been met do not be afraid to reschedule or cancel altogether.</p>
<h2 id="the-alternative-the-ad-hoc-huddle">The Alternative: The Ad Hoc Huddle</h2>
<p>The rules above do not mean that you cannot talk to one another without ritual. Especially for small discussions between 2-4 people, ad hoc huddles can be useful.</p>
<p>As a rule of thumb, these should usually be happening within the next 24 hours, so usually “later today” or “tomorrow morning”.</p>
<p>You still need to define a goal, and you will want to write down any outcomes in some form, even if less formal than meeting notes. Even a Slack message with some findings in a relevant channel counts.</p>]]></summary>
</entry>

</feed>
